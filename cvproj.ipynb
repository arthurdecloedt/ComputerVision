{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hq-TbRYlHDl",
    "colab_type": "text"
   },
   "source": [
    "# Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0odpzVCNvtU_",
    "colab_type": "code",
    "outputId": "3537aa69-5712-4303-9b7c-933beec65a87",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow) (0.46)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ctH2yd97Oz3e",
    "colab_type": "code",
    "outputId": "f7bd2a60-cd37-4a87-f5fa-1fd164fc9f5e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sync at Sat, 22 Jun 2019 18:13:33 +0000\n"
     ]
    },
    {
     "metadata": {},
     "text": []
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "from google.colab import drive\n",
    "\n",
    "print(\"sync at \" + strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime()))\n",
    "drive.mount('/content/drive')\n",
    "image_size = 208    # image size that you will use for your network (input images will be resampled to this size), lower if you have troubles on your laptop (hint: use io.imshow to inspect the quality of the resampled images before feeding it into your network!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fspPaE-0Gne_",
    "colab_type": "text"
   },
   "source": [
    "**If regenerating dataset: download dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "aWmmChtfB7CG",
    "colab_type": "code",
    "outputId": "b06ca92f-24b0-447e-ba61-44cc312d80ab",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-15 15:40:13--  http://host.robots.ox.ac.uk/pascal/VOC/voc2009/VOCtrainval_11-May-2009.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 935534080 (892M) [application/x-tar]\n",
      "Saving to: ‘VOCtrainval_11-May-2009.tar’\n",
      "\n",
      "VOCtrainval_11-May- 100%[===================>] 892.19M  13.3MB/s    in 66s     \n",
      "\n",
      "2019-06-15 15:41:20 (13.4 MB/s) - ‘VOCtrainval_11-May-2009.tar’ saved [935534080/935534080]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://host.robots.ox.ac.uk/pascal/VOC/voc2009/VOCtrainval_11-May-2009.tar\n",
    "!tar -xf \"/content/VOCtrainval_11-May-2009.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnrgM3YUG9Bs",
    "colab_type": "text"
   },
   "source": [
    "Getting pregenerated dataset from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6nFOY6yrU88b",
    "colab_type": "code",
    "outputId": "41f802c4-3fa4-4528-cd97-08cf20bb3d87",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sync at Sat, 22 Jun 2019 20:49:04 +0000\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "from google.colab import drive\n",
    "\n",
    "print(\"sync at \" + strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime()))\n",
    "drive.mount('/content/drive')\n",
    "image_size = 208    # image size that you will use for your network (input images will be resampled to this size), lower if you have troubles on your laptop (hint: use io.imshow to inspect the quality of the resampled images before feeding it into your network!)\n",
    "\n",
    "from lxml import etree\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import threshold_otsu\n",
    "from tensorflow.keras.layers import Flatten, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "input_img = Input(shape=(image_size, image_size, 3)) \n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "\n",
    "x_train = np.load(\"/content/drive/My Drive/cvproj/dataset/x_train.npy\")\n",
    "y_train = np.load(\"/content/drive/My Drive/cvproj/dataset/y_train.npy\")\n",
    "x_strain = np.load(\"/content/drive/My Drive/cvproj/dataset/x_strain.npy\")\n",
    "y_strain = np.load(\"/content/drive/My Drive/cvproj/dataset/y_strain.npy\")\n",
    "x_val = np.load(\"/content/drive/My Drive/cvproj/dataset/x_val.npy\")\n",
    "y_val = np.load(\"/content/drive/My Drive/cvproj/dataset/y_val.npy\")\n",
    "x_sval = np.load(\"/content/drive/My Drive/cvproj/dataset/x_sval.npy\")\n",
    "y_sval = np.load(\"/content/drive/My Drive/cvproj/dataset/y_sval.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BOFzEI-1dqpa",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445.0
    },
    "outputId": "dd330412-d838-40a6-ff1d-9f48c3e380b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorboard as it is not installed.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
      "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.6/dist-packages (1.14.1.dev20190622)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.16.4)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.7.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
      "Requirement already satisfied: tb-nightly<1.15.0a0,>=1.14.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.14.0a20190614)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.2)\n",
      "Requirement already satisfied: tf-estimator-nightly in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.14.0.dev2019062201)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.21.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.11.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.1.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.8.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.33.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.0.8)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly) (41.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly) (0.15.4)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly) (2.9.0)\n",
      "The tensorboard module is not an IPython extension.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "xTKwBzjCa_x7",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "c1a4a8d9-05ff-4750-b885-ef85942965bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUqcAL58cygr",
    "colab_type": "text"
   },
   "source": [
    "Get tensorboard support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKdg64TgHCCl",
    "colab_type": "text"
   },
   "source": [
    "Regenerating dataset: only if downloaded fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "aMC0dVimZN9C",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "input_img = Input(shape=(image_size, image_size, 3)) \n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "JsD2SbaYZIiU",
    "colab_type": "code",
    "outputId": "0ace998b-871e-43fd-cdae-aa3b573e849f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463 training images from 2 classes\n",
      "449 validation images from 2 classes\n",
      "749 training pairs\n",
      "750 validation pairs\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "# parameters that you should set before running this script\n",
    "filter = ['aeroplane', 'bird']       # select class, this default should yield 1489 training and 1470 validation images\n",
    "voc_root_folder = \"/content/VOCdevkit\"  # please replace with the location on your laptop where you unpacked the tarball\n",
    "image_size = 208    # image size that you will use for your network (input images will be resampled to this size), lower if you have troubles on your laptop (hint: use io.imshow to inspect the quality of the resampled images before feeding it into your network!)\n",
    "\n",
    "\n",
    "# step1 - build list of filtered filenames\n",
    "annotation_folder = os.path.join(voc_root_folder, \"VOC2009/Annotations/\")\n",
    "annotation_files = os.listdir(annotation_folder)\n",
    "filtered_filenames = []\n",
    "for a_f in annotation_files:\n",
    "    tree = etree.parse(os.path.join(annotation_folder, a_f))\n",
    "    if np.any([tag.text == filt for tag in tree.iterfind(\".//name\") for filt in filter]):\n",
    "        filtered_filenames.append(a_f[:-4])\n",
    "\n",
    "# step2 - build (x,y) for TRAIN/VAL (classification)\n",
    "classes_folder = os.path.join(voc_root_folder, \"VOC2009/ImageSets/Main/\")\n",
    "classes_files = os.listdir(classes_folder)\n",
    "train_files = [os.path.join(classes_folder, c_f) for filt in filter for c_f in classes_files if filt in c_f and '_train.txt' in c_f]\n",
    "val_files = [os.path.join(classes_folder, c_f) for filt in filter for c_f in classes_files if filt in c_f and '_val.txt' in c_f]\n",
    "\n",
    "\n",
    "def build_classification_dataset(list_of_files):\n",
    "    \"\"\" build training or validation set\n",
    "\n",
    "    :param list_of_files: list of filenames to build trainset with\n",
    "    :return: tuple with x np.ndarray of shape (n_images, image_size, image_size, 3) and  y np.ndarray of shape (n_images, n_classes)\n",
    "    \"\"\"\n",
    "    temp = []\n",
    "    train_labels = []\n",
    "    for f_cf in list_of_files:\n",
    "        with open(f_cf) as file:\n",
    "            lines = file.read().splitlines()\n",
    "            temp.append([line.split()[0] for line in lines if int(line.split()[-1]) == 1])\n",
    "            label_id = [f_ind for f_ind, filt in enumerate(filter) if filt in f_cf][0]\n",
    "            train_labels.append(len(temp[-1]) * [label_id])\n",
    "    train_filter = [item for l in temp for item in l]\n",
    "\n",
    "    image_folder = os.path.join(voc_root_folder, \"VOC2009/JPEGImages/\")\n",
    "    image_filenames = [os.path.join(image_folder, file) for f in train_filter for file in os.listdir(image_folder) if\n",
    "                       f in file]\n",
    "    x = np.array([resize(io.imread(img_f,0), (image_size, image_size,3)) for img_f in image_filenames]).astype(\n",
    "        'float32')\n",
    "    # changed y to an array of shape (num_examples, num_classes) with 0 if class is not present and 1 if class is present\n",
    "    y_temp = []\n",
    "    for tf in train_filter:\n",
    "        y_temp.append([1 if tf in l else 0 for l in temp])\n",
    "    y = np.array(y_temp)\n",
    "\n",
    "    return x, y\n",
    "def build_seg_dataset(list_of_files):\n",
    "  \n",
    "    image_folder = os.path.join(voc_root_folder, \"VOC2009/JPEGImages/\")\n",
    "    mask_folder = os.path.join(voc_root_folder, \"VOC2009/SegmentationClass/\")\n",
    "    image_filenames = [os.path.join(image_folder, file) for f in list_of_files for file in os.listdir(image_folder) if\n",
    "                       f in file]\n",
    "    mask_filenames = [os.path.join(mask_folder, file) for f in list_of_files for file in os.listdir(mask_folder) if\n",
    "                       f in file]\n",
    "    x = np.array([resize(io.imread(img_f,0), (image_size, image_size)) for img_f in image_filenames]).astype('float32')\n",
    "    y = np.array([resize(io.imread(img_f,1), (image_size, image_size)) > 0.01 for img_f in mask_filenames]).astype('float32')\n",
    "    return x,y\n",
    "x_train, y_train = build_classification_dataset(train_files)\n",
    "print('%i training images from %i classes' %(x_train.shape[0], y_train.shape[1]))\n",
    "x_val, y_val = build_classification_dataset(val_files)\n",
    "print('%i validation images from %i classes' %(x_val.shape[0],  y_train.shape[1]))\n",
    "# x_strain, y_strain = build_seg_dataset(open(list_train).read().splitlines())\n",
    "print('%i training pairs' %(x_strain.shape[0]))\n",
    "# x_sval, y_sval = build_seg_dataset(open(list_val).read().splitlines())\n",
    "print('%i validation pairs' %(x_sval.shape[0]))\n",
    "\n",
    "\n",
    "# from here, you can start building your model open(list_train).read().splitlines()\n",
    "# you will only need x_train and x_val for the autoencoder\n",
    "# you should extend the above script for the segmentation task (you will need a slightly different function for building the label images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Znf8nLfoHKRR",
    "colab_type": "text"
   },
   "source": [
    "Saving dataset to drive, only if fresh and regenerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ZMj5D6QyUtYR",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "np.save(\"/content/drive/My Drive/cvproj/dataset/x_train\",x_train)\n",
    "np.save(\"/content/drive/My Drive/cvproj/dataset/y_train\",y_train)\n",
    "np.save(\"/content/drive/My Drive/cvproj/dataset/x_strain\",x_strain)\n",
    "np.save(\"/content/drive/My Drive/cvproj/dataset/y_strain\",y_strain)\n",
    "np.save(\"/content/drive/My Drive/cvproj/dataset/x_val\",x_val)\n",
    "np.save(\"/content/drive/My Drive/cvproj/dataset/y_val\",y_val)\n",
    "np.save(\"/content/drive/My Drive/cvproj/dataset/x_sval\",x_sval)\n",
    "np.save(\"/content/drive/My Drive/cvproj/dataset/y_sval\",y_sval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "R9iyoRpmaoE6",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!rsync -a  --no-compress --progress \"/content/VOCdevkit\" \"/content/drive/My Drive/cvproj/dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P6q4aq5HfBJ",
    "colab_type": "text"
   },
   "source": [
    "Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1Jcqy75elDMt",
    "colab_type": "code",
    "outputId": "f321b3a2-b6f1-439d-9bca-831840935ab2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 208, 208, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 208, 208, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 104, 104, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 104, 104, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 104, 104, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 52, 52, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 13, 13, 32)        36896     \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 26, 26, 64)        18496     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 52, 52, 64)        36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 104, 104, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 104, 104, 32)      18464     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 104, 104, 32)      9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 208, 208, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 208, 208, 16)      4624      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 208, 208, 3)       435       \n",
      "=================================================================\n",
      "Total params: 412,099\n",
      "Trainable params: 412,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "K.clear_session()\n",
    "image_size=208\n",
    "input_img = Input(shape=(image_size, image_size, 3)) \n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same',input_shape=(image_size, image_size, 3))(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "encoded1 = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(encoded1)\n",
    "encoded2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded =Conv2D(128,(3, 3), strides=(2,2), padding='same')(encoded2)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu',padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu',padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3,3),activation='relu',padding='same')(x)\n",
    "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "for layer in autoencoder.layers:\n",
    "   \n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "a3VE3juOlGCY",
    "colab_type": "code",
    "outputId": "dc3c2c7b-35bf-4635-bf89-e1b5c0a5f209",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1489 samples, validate on 1470 samples\n",
      "Epoch 1/100\n",
      "1360/1489 [==========================>...] - ETA: 0s - loss: 0.6144"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/api/_v1/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m autoencoder.fit( x_train,x_train,batch_size=16,shuffle=True,epochs=100,\n\u001b[1;32m      9\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 callbacks=[TensorBoard(log_dir='/tmp/autoencoder'),es,mc])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "mc = ModelCheckpoint('best_modelautoenc.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
    "autoencoder.fit( x_train,x_train,batch_size=16,shuffle=True,epochs=100,\n",
    "                validation_data=(x_val, x_val),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder'),es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "HG-FxwbsoKQt",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "autoencoder.load_weights(\"/content/drive/My Drive/cvproj/model_out/model_out/outoencoder150epoch.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ6fIOknlD_u",
    "colab_type": "text"
   },
   "source": [
    "Visualize autoenc output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "W_QqDIb6Jx9G",
    "colab_type": "code",
    "outputId": "eddcfdcb-d31f-4328-8c81-d6759e31ca8f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1703.0
    }
   },
   "outputs": [
    {
     "metadata": {},
     "text": []
    }
   ],
   "source": [
    "import scipy\n",
    "from PIL import Image\n",
    "import imageio\n",
    "full =np.array(x_val, copy=True)\n",
    "np.random.shuffle(full)\n",
    "sel = full[:50]\n",
    "\n",
    "decoded_imgs = autoencoder.predict(sel)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "n = 50\n",
    "for i in range(1,n):\n",
    "    imageio.imwrite(\"/content/drive/My Drive/cvproj/outfigautenc/\"+ str(i)+\"in.png\",sel[i])\n",
    "    imageio.imwrite(\"/content/drive/My Drive/cvproj/outfigautenc/\"+ str(i)+\"out.png\",decoded_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "aE3FYHYtesG1",
    "colab_type": "code",
    "outputId": "c454f2b6-8ca8-4f2a-fade-1801cab35615",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "metadata": {},
     "text": []
    }
   ],
   "source": [
    "full =np.array(x_val, copy=True)\n",
    "np.random.shuffle(full)\n",
    "sel = full[:10]\n",
    "\n",
    "decoded_imgs = autoencoder.predict(sel)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1,n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(sel[i].reshape(208, 208,3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(208, 208,3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "plt.savefig(\"/content/drive/My Drive/cvproj/out.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vrEsMP3OqgRm",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "9d69234b-6cc3-4c67-d813-c9335f695a24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir /tmp/autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "U1ZK33md35t9",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!mkdir -p /model_out\n",
    "model_json = autoencoder.to_json()\n",
    "with open(\"/model_out/outoencoder150.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "autoencoder.save_weights(\"/model_out/outoencoder150epoch.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "zAA9tQLxgffY",
    "colab_type": "code",
    "outputId": "0667ecdd-5d93-4367-eea6-2d919ca70888",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending incremental file list\n",
      "model_out/\n",
      "model_out/outoencoder150.json\n",
      "\r         10,141 100%    0.00kB/s    0:00:00  \r         10,141 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=1/3)\n",
      "model_out/outoencoder150epoch.h5\n",
      "\r         32,768   3%   31.25MB/s    0:00:00  \r        923,240 100%   58.70MB/s    0:00:00 (xfr#2, to-chk=0/3)\n",
      "\n",
      "sent 933,803 bytes  received 58 bytes  1,867,722.00 bytes/sec\n",
      "total size is 933,381  speedup is 1.00\n"
     ]
    }
   ],
   "source": [
    "!rsync -a -v --no-compress --progress \"/model_out\" \"/content/drive/My Drive/cvproj/model_out/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "rSStgKPf4Uz3",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "! rm -r /tmp/classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i8JY0dDcgfbi",
    "colab_type": "code",
    "outputId": "980c473c-e1bf-4c7b-e258-763fd970da50",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 208, 208, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 208, 208, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 104, 104, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 104, 104, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 104, 104, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 52, 52, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 21632)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 108165    \n",
      "=================================================================\n",
      "Total params: 395,173\n",
      "Trainable params: 395,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "#classif = Sequential()\n",
    "l = Dropout(rate=0.5)(encoded)\n",
    "#outl = Conv2D(y_train.shape[1],(13,13),activation='softmax',name=\"fcimit\")(l)\n",
    "#outl = Reshape((5,))(outl)\n",
    "l=Flatten()(l)\n",
    "outl = Dense(5,activation='softmax')(l)\n",
    "classif= Model(input_img,outl)\n",
    "for layer in classif.layers[:10]:\n",
    "  layer.trainable = True\n",
    "\n",
    "\n",
    "classif.compile(optimizer=optimizers.SGD(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "classif.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "pjGA88vn5Hg3",
    "colab_type": "code",
    "outputId": "eef0283f-2a94-4656-9582-82c5b1551628",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198.0
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-083b7828356f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kernel_initializer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'run'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for layer in classif.layers: \n",
    "      if hasattr(layer, 'kernel_initializer'):\n",
    "          layer.kernel.initializer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "eLYDjZ2OnkWl",
    "colab_type": "code",
    "outputId": "ef6436f4-0cdb-40fe-a974-d5928c656d30",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1489 samples, validate on 1470 samples\n",
      "Epoch 1/1000\n",
      "1488/1489 [============================>.] - ETA: 0s - loss: 1.6950 - acc: 0.2487\n",
      "Epoch 00001: val_acc improved from -inf to 0.22925, saving model to /content/drive/My Drive/cvproj/best_modelclassifprofull2a.h5\n",
      "1489/1489 [==============================] - 157s 106ms/sample - loss: 1.6959 - acc: 0.2485 - val_loss: 1.7400 - val_acc: 0.2293\n",
      "Epoch 2/1000\n",
      "1488/1489 [============================>.] - ETA: 0s - loss: 1.7616 - acc: 0.2520"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=200)\n",
    "mc = ModelCheckpoint('/content/drive/My Drive/cvproj/best_modelclassifprofull2a.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)\n",
    "classif.fit(x_train, y_train, batch_size=16, epochs=1000, shuffle=True,\n",
    "                validation_data=(x_val, y_val),\n",
    "                callbacks=[TensorBoard(log_dir='/content/drive/My Drive/cvproj/classif'),es,mc])\n",
    "\n",
    "\n",
    "#full =np.array(x_val, copy=True)\n",
    "#np.random.shuffle(full)\n",
    "#sel = full[:100]\n",
    "#classifm=Sequential()\n",
    "#classifm.add(classif)\n",
    "#import time\n",
    "#decoded = classifm.predict_classes(sel,verbose=1)\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.ioff()\n",
    "\n",
    "#filter = ['aeroplane','bird']\n",
    "#n = 100\n",
    "#for i in range(1,n):\n",
    "#  fig = plt.figure(figsize=(4, 4))\n",
    "#  plt.imshow(sel[i].reshape(208, 208,3))\n",
    "#  plt.gray()\n",
    "#  plt.xlabel(filter[decoded[i]]) \n",
    "#  ax=plt.gca()\n",
    "#  ax.get_yaxis().set_visible(False)\n",
    "#  ax.get_xaxis().set_ticks([])\n",
    "#  ax.get_yaxis().set_ticklabels([])\n",
    "#  plt.savefig(\"/content/drive/My Drive/cvproj/outfigclassif/\" +str(i)+ \".png\")\n",
    "#  plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "uqqJMRQnEo_e",
    "colab_type": "code",
    "outputId": "78005f54-4406-4878-ce92-4b36e818424a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 4ms/sample\n"
     ]
    }
   ],
   "source": [
    "full =np.array(x_val, copy=True)\n",
    "np.random.shuffle(full)\n",
    "sel = full[:100]\n",
    "classifm=Sequential()\n",
    "classifm.add(classif)\n",
    "import time\n",
    "decoded = classifm.predict_classes(sel,verbose=1)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "\n",
    "filter = ['aeroplane','bird']\n",
    "n = 100\n",
    "for i in range(1,n):\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "  plt.imshow(sel[i].reshape(208, 208,3))\n",
    "  plt.gray()\n",
    "  plt.xlabel(filter[decoded[i]]) \n",
    "  ax=plt.gca()\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "  ax.get_xaxis().set_ticks([])\n",
    "  ax.get_yaxis().set_ticklabels([])\n",
    "  plt.savefig(\"/content/drive/My Drive/cvproj/outfigclassif/\" +str(i)+ \".png\")\n",
    "  plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "AKvf2QmIcULn",
    "colab_type": "code",
    "outputId": "a4804021-6945-46ee-d1e5-4a03c3e6785e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 25ms/sample\n",
      "[4 3 2 1 2 4 1 1 3 4 1 0 1 1 4 3 3 1 3 3 4 2 0 3 1 1 2 1 1 4 2 1 2 1 0 1 1\n",
      " 1 0 1]\n"
     ]
    },
    {
     "metadata": {},
     "text": []
    }
   ],
   "source": [
    "classif.load_weights(\"/content/drive/My Drive/cvproj/best_modelclassifpro.h5\")\n",
    "full =np.array(x_val, copy=True)\n",
    "np.random.shuffle(full)\n",
    "sel = full[:40]\n",
    "classifm=Sequential()\n",
    "classifm.add(classif)\n",
    "filter = ['aeroplane', 'car', 'chair', 'dog', 'bird']\n",
    "decoded = classifm.predict_classes(sel,verbose=1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1,n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(sel[i].reshape(208, 208,3))\n",
    "    plt.gray()\n",
    "    plt.xlabel(filter[decoded[i]]) \n",
    "    \n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    ax.get_yaxis().set_ticklabels([])\n",
    "    \n",
    "    ax = plt.subplot(2, n, n+i)\n",
    "    plt.imshow(sel[i+20].reshape(208, 208,3))\n",
    "    plt.gray()\n",
    "    plt.xlabel(filter[decoded[i+20]]) \n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    ax.get_yaxis().set_ticklabels([])\n",
    "\n",
    "\n",
    "plt.savefig(\"/content/drive/My Drive/cvproj/out\" + \".jpg\")\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "2F1XXNDHoLmm",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!mkdir -p /model_out\n",
    "model_json = classif.to_json()\n",
    "with open(\"/model_out/classif400_50drop.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "classif.save_weights(\"/model_out/classif400_50drop.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "taUHfmXItC3p",
    "colab_type": "code",
    "outputId": "019fe88a-c0a2-489c-dd8d-c518db771534",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1091.0
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0622 19:02:56.548506 140081001686912 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:182: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 208, 208, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 208, 208, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 208, 208, 64) 36928       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 104, 104, 64) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 104, 104, 128 73856       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 104, 104, 128 147584      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 52, 52, 128)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 52, 52, 256)  295168      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 26, 26, 256)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 26, 26, 512)  1180160     max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 26, 26, 512)  2359808     conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 52, 52, 512)  0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 52, 52, 768)  0           conv2d_5[0][0]                   \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 52, 52, 256)  1769728     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 52, 52, 256)  590080      conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 104, 104, 256 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 104, 104, 384 0           conv2d_3[0][0]                   \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 104, 104, 128 442496      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 104, 104, 128 147584      conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 208, 208, 128 0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 208, 208, 192 0           conv2d_1[0][0]                   \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 208, 208, 64) 110656      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 208, 208, 64) 36928       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 208, 208, 1)  65          conv2d_13[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,192,833\n",
      "Trainable params: 7,192,833\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "image_size=208\n",
    "K.clear_session()\n",
    "sinput_img = Input(shape=(image_size, image_size, 3)) \n",
    "sx = Conv2D(64, (3, 3), activation='relu', padding='same',input_shape=(image_size, image_size, 3))(sinput_img)\n",
    "sxb = Conv2D(64, (3,3),activation='relu',padding='same')(sx)\n",
    "sx = MaxPooling2D((2, 2), padding='same')(sxb)\n",
    "sx1 = Conv2D(128, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx1 = Conv2D(128, (3, 3), activation='relu', padding='same')(sx1)\n",
    "sx = MaxPooling2D((2, 2), padding='same')(sx1)\n",
    "sx2 = Dropout(rate=.5)(sx)\n",
    "sx2 = Conv2D(256, (3, 3), activation='relu', padding='same')(sx2)\n",
    "sx2 = Conv2D(256, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = MaxPooling2D((2, 2), padding='same')(sx2)\n",
    "sx = Conv2D(512, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = Conv2D(512, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = UpSampling2D((2, 2))(sx)\n",
    "sx = concatenate([sx2,sx], axis = 3)\n",
    "sx = Conv2D(256, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = Conv2D(256, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = UpSampling2D((2, 2))(sx)\n",
    "sx = concatenate([sx1,sx], axis = 3)\n",
    "sx = Conv2D(128, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = Conv2D(128, (3, 3), activation='relu',padding='same')(sx)\n",
    "sx = UpSampling2D((2, 2))(sx)\n",
    "sx = concatenate([sxb,sx], axis = 3)\n",
    "sx = Conv2D(64, (3,3),activation='relu',padding='same')(sx)\n",
    "sx = Conv2D(64, (3,3),activation='relu',padding='same')(sx)\n",
    "seg = Conv2D(1, (1, 1), activation='sigmoid', padding='same')(sx)\n",
    "segautoenc = Model(sinput_img,seg)\n",
    "\n",
    "\n",
    "segautoenc.compile(optimizers.Adam(lr=0.0001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "segautoenc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "oY3Y6t4xmpxk",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "segautoenc.load_weights(\"/content/drive/My Drive/cvproj/model_out/Useg-20.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "y8Yt0x_3Xcd-",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import *\n",
    "class RecordImg(Callback):\n",
    "    def __init__(self,path=\"/content/drive/My Drive/cvproj/out\",freq=5):\n",
    "      if path[-1]!='/':\n",
    "        path += '/'\n",
    "      self.freq=freq\n",
    "      self.path=path\n",
    "    def on_epoch_end(self,epoch, logs={}):\n",
    "        if epoch %self.freq != 0 :\n",
    "          return\n",
    "        model=self.model\n",
    "        imnr=(2*epoch) % 700\n",
    "        img = x_sval[imnr:imnr+1]\n",
    "        imgs = y=y_sval[imnr]\n",
    "        out = model.predict(img)\n",
    "        imageio.imwrite(self.path+\"epoch\"+ str(epoch)+\"in.png\",img_as_ubyte(img[0]))\n",
    "        imageio.imwrite(self.path+\"epoch\"+ str(epoch)+\"out.png\",img_as_ubyte(out[0]))\n",
    "        imageio.imwrite(self.path+\"epoch\"+ str(epoch)+\"ground.png\",img_as_ubyte(imgs))\n",
    "    \n",
    "def getGen(x,y):\n",
    "    data_gen_args = dict(featurewise_center=True,\n",
    "                         featurewise_std_normalization=True,\n",
    "                         rotation_range=20,\n",
    "                         width_shift_range=0.1,\n",
    "                         height_shift_range=0.1,\n",
    "                         \n",
    "                         zoom_range=0.2)\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
    "    seed = 1\n",
    "    image_datagen.fit(x, augment=True, seed=seed)\n",
    "    mask_datagen.fit(y[...,np.newaxis], augment=True, seed=seed)\n",
    "\n",
    "    image_generator = image_datagen.flow(\n",
    "        x,\n",
    "        batch_size = 16,\n",
    "        seed=seed)\n",
    "\n",
    "    mask_generator = mask_datagen.flow(\n",
    "        y[...,np.newaxis],\n",
    "        batch_size = 16,\n",
    "        seed=seed)\n",
    "    while True:\n",
    "        mask = mask_generator.next()\n",
    "        mask[mask > 0.5] = 1\n",
    "        mask[mask <= 0.5] = 0\n",
    "        yield (image_generator.next(),mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "L_ULT7hTvFyu",
    "colab_type": "code",
    "outputId": "e10e9fb8-b36b-448f-997c-1aa559a1cfe2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2434.0
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0618 22:44:56.470298 140121062393728 callbacks.py:875] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3950 - acc: 0.8466\n",
      "Epoch 00001: val_acc improved from -inf to 0.78927, saving model to /content/drive/My Drive/cvproj/best_modelsegUjdl.hdf5\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.3928 - acc: 0.8474 - val_loss: 0.5429 - val_acc: 0.7893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:135: UserWarning: Possible precision loss when converting from float32 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2560 - acc: 0.8954\n",
      "Epoch 00002: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 81s 807ms/step - loss: 0.2562 - acc: 0.8952 - val_loss: 0.6677 - val_acc: 0.7679\n",
      "Epoch 3/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9009\n",
      "Epoch 00003: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 81s 813ms/step - loss: 0.2463 - acc: 0.9013 - val_loss: 0.6430 - val_acc: 0.7755\n",
      "Epoch 4/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9090\n",
      "Epoch 00004: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 80s 795ms/step - loss: 0.2313 - acc: 0.9094 - val_loss: 0.6757 - val_acc: 0.7821\n",
      "Epoch 5/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9143\n",
      "Epoch 00005: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 784ms/step - loss: 0.2195 - acc: 0.9145 - val_loss: 0.7302 - val_acc: 0.7713\n",
      "Epoch 6/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9165\n",
      "Epoch 00006: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 79s 788ms/step - loss: 0.2225 - acc: 0.9165 - val_loss: 0.6994 - val_acc: 0.7752\n",
      "Epoch 7/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9245\n",
      "Epoch 00007: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 79s 790ms/step - loss: 0.2001 - acc: 0.9246 - val_loss: 0.6650 - val_acc: 0.7804\n",
      "Epoch 8/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9252\n",
      "Epoch 00008: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 79s 786ms/step - loss: 0.1999 - acc: 0.9253 - val_loss: 0.6815 - val_acc: 0.7774\n",
      "Epoch 9/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9275\n",
      "Epoch 00009: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 784ms/step - loss: 0.1949 - acc: 0.9278 - val_loss: 0.7032 - val_acc: 0.7844\n",
      "Epoch 10/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9251\n",
      "Epoch 00010: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 783ms/step - loss: 0.2030 - acc: 0.9254 - val_loss: 0.7121 - val_acc: 0.7774\n",
      "Epoch 11/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9303\n",
      "Epoch 00011: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 79s 790ms/step - loss: 0.1894 - acc: 0.9303 - val_loss: 0.6974 - val_acc: 0.7779\n",
      "Epoch 12/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.9368\n",
      "Epoch 00012: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 783ms/step - loss: 0.1752 - acc: 0.9369 - val_loss: 0.7346 - val_acc: 0.7813\n",
      "Epoch 13/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9335\n",
      "Epoch 00013: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 79s 788ms/step - loss: 0.1858 - acc: 0.9335 - val_loss: 0.7085 - val_acc: 0.7753\n",
      "Epoch 14/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9347\n",
      "Epoch 00014: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 782ms/step - loss: 0.1812 - acc: 0.9350 - val_loss: 0.6830 - val_acc: 0.7860\n",
      "Epoch 15/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9349\n",
      "Epoch 00015: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 79s 789ms/step - loss: 0.1823 - acc: 0.9352 - val_loss: 0.7404 - val_acc: 0.7632\n",
      "Epoch 16/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9397\n",
      "Epoch 00016: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 782ms/step - loss: 0.1706 - acc: 0.9397 - val_loss: 0.6968 - val_acc: 0.7792\n",
      "Epoch 17/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9396\n",
      "Epoch 00017: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 79s 786ms/step - loss: 0.1708 - acc: 0.9398 - val_loss: 0.7388 - val_acc: 0.7806\n",
      "Epoch 18/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9372\n",
      "Epoch 00018: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 784ms/step - loss: 0.1762 - acc: 0.9374 - val_loss: 0.7186 - val_acc: 0.7805\n",
      "Epoch 19/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9403\n",
      "Epoch 00019: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 79s 787ms/step - loss: 0.1714 - acc: 0.9406 - val_loss: 0.7617 - val_acc: 0.7691\n",
      "Epoch 20/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9396\n",
      "Epoch 00020: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 783ms/step - loss: 0.1723 - acc: 0.9398 - val_loss: 0.7734 - val_acc: 0.7831\n",
      "Epoch 21/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9374\n",
      "Epoch 00021: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 784ms/step - loss: 0.1776 - acc: 0.9377 - val_loss: 0.6997 - val_acc: 0.7740\n",
      "Epoch 22/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9411\n",
      "Epoch 00022: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 782ms/step - loss: 0.1684 - acc: 0.9413 - val_loss: 0.7617 - val_acc: 0.7792\n",
      "Epoch 23/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9485\n",
      "Epoch 00023: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 784ms/step - loss: 0.1528 - acc: 0.9487 - val_loss: 0.7634 - val_acc: 0.7830\n",
      "Epoch 24/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9459\n",
      "Epoch 00024: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 779ms/step - loss: 0.1581 - acc: 0.9461 - val_loss: 0.7785 - val_acc: 0.7764\n",
      "Epoch 25/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9463\n",
      "Epoch 00025: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 784ms/step - loss: 0.1588 - acc: 0.9465 - val_loss: 0.7683 - val_acc: 0.7725\n",
      "Epoch 26/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9424\n",
      "Epoch 00026: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 783ms/step - loss: 0.1696 - acc: 0.9426 - val_loss: 0.7051 - val_acc: 0.7796\n",
      "Epoch 27/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9485\n",
      "Epoch 00027: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 79s 786ms/step - loss: 0.1508 - acc: 0.9487 - val_loss: 0.7772 - val_acc: 0.7788\n",
      "Epoch 28/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9479\n",
      "Epoch 00028: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 782ms/step - loss: 0.1588 - acc: 0.9481 - val_loss: 0.7775 - val_acc: 0.7752\n",
      "Epoch 29/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9452\n",
      "Epoch 00029: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 783ms/step - loss: 0.1614 - acc: 0.9456 - val_loss: 0.7296 - val_acc: 0.7831\n",
      "Epoch 30/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9508\n",
      "Epoch 00030: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 783ms/step - loss: 0.1477 - acc: 0.9509 - val_loss: 0.7888 - val_acc: 0.7739\n",
      "Epoch 31/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9449\n",
      "Epoch 00031: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 785ms/step - loss: 0.1648 - acc: 0.9450 - val_loss: 0.7383 - val_acc: 0.7832\n",
      "Epoch 32/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9478\n",
      "Epoch 00032: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 780ms/step - loss: 0.1572 - acc: 0.9479 - val_loss: 0.7720 - val_acc: 0.7745\n",
      "Epoch 33/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9517\n",
      "Epoch 00033: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 783ms/step - loss: 0.1454 - acc: 0.9519 - val_loss: 0.8187 - val_acc: 0.7851\n",
      "Epoch 34/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1532 - acc: 0.9498\n",
      "Epoch 00034: val_acc did not improve from 0.78927\n",
      "100/100 [==============================] - 78s 778ms/step - loss: 0.1526 - acc: 0.9499 - val_loss: 0.7674 - val_acc: 0.7802\n",
      "Epoch 35/50\n",
      "  2/100 [..............................] - ETA: 48s - loss: 0.3543 - acc: 0.9100"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "csv = CSVLogger('/content/drive/My Drive/cvproj/seglogd.csv',append=True)\n",
    "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=100)\n",
    "mc = ModelCheckpoint('/content/drive/My Drive/cvproj/best_modelsegUjdl.hdf5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)\n",
    "mc2 = ModelCheckpoint('/content/drive/My Drive/cvproj/model_out/Usegjdl-{epoch:02d}.hdf5',period=1)\n",
    "ri= RecordImg(path=\"/content/drive/My Drive/cvproj/outseg\",freq=1)\n",
    "traingen=getGen(x_strain,y_strain)\n",
    "valgen=getGen(x_sval,y_sval)\n",
    "segautoenc.fit_generator(\n",
    "                traingen,steps_per_epoch=100,epochs=50,\n",
    "                validation_data=valgen,validation_steps=50,\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/seg'),es,mc,mc2,ri,csv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "oAl9ciXxoxiU",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503.0
    },
    "outputId": "9d768b52-ca62-4c26-d825-8d6e13842433"
   },
   "outputs": [],
   "source": [
    "index = np.random.randint(100,size=10)\n",
    "n = 9\n",
    "plt.figure(figsize=(20, 8))\n",
    "print(index)\n",
    "for i in range(1,n):\n",
    "    \n",
    "    # display original\n",
    "    ax = plt.subplot(4, n, i)\n",
    "    plt.imshow(x_sval[index[i]].reshape(208, 208,3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    # display original\n",
    "    ax = plt.subplot(4, n, i+n)\n",
    "    plt.imshow(y_sval[index[i]].reshape(208, 208))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(4, n, i + 2*n)\n",
    "    plt.imshow(res[index[i]].reshape(208, 208))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(4, n, i + 3*n)\n",
    "    plt.imshow(res[index[i]].reshape(208, 208)>threshold_otsu(res[index[i]]))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    \n",
    "\n",
    "plt.savefig(\"/content/drive/My Drive/cvproj/outseg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "WIoAETCc5wNX",
    "colab_type": "code",
    "outputId": "ae9c6878-febe-4196-b52c-9ffe9427e9e8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395.0
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/api/_v1/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfull\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegautoenc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Setup work for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_training_eval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[0;31m# Reset metrics on all the distributed (cloned) models.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \"\"\"\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3109\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 941\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    942\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1164\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1165\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1342\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1343\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1331\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1333\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1425\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "segautoenc=load_model(\"/content/drive/My Drive/cvproj/model_out/Useg-20.hdf5\")\n",
    "full =np.array(x_sval, copy=True)\n",
    "sel = full[0:100]\n",
    "res=segautoenc.predict(sel)\n",
    "index = np.random.randint(100,size=10)\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(1,n):\n",
    "    \n",
    "    # display original\n",
    "    ax = plt.subplot(4, n, i)\n",
    "    plt.imshow(x_sval[index[i]].reshape(208, 208,3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    # display original\n",
    "    ax = plt.subplot(4, n, i+n)\n",
    "    plt.imshow(y_sval[index[i]].reshape(208, 208))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(4, n, i + 2*n)\n",
    "    plt.imshow(res[index[i]].reshape(208, 208))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(4, n, i + 3*n)\n",
    "    plt.imshow(res[index[i]].reshape(208, 208)>threshold_otsu(res[index[i]]))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    \n",
    "plt.show()\n",
    "plt.savefig(\"/content/drive/My Drive/cvproj/outse.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "bnV9Wa_iwGOn",
    "colab_type": "code",
    "outputId": "fc6d9c65-a91a-4cdf-919d-2c9ade2bcd16",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340.0
    }
   },
   "outputs": [],
   "source": [
    "list_train = os.path.join(voc_root_folder, \"VOC2009/ImageSets/Segmentation/train.txt\")\n",
    "list_val = os.path.join(voc_root_folder, \"VOC2009/ImageSets/Segmentation/val.txt\")\n",
    "print(open(list_train).read().splitlines())\n",
    "len(list_train)\n",
    "print(train_files)\n",
    "plt.imshow(y_strain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nK0yENvf9yP0",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164.0
    },
    "outputId": "9410082c-bb55-461b-cd21-8234126777d6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-16a226908a07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msegautoenc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/cvproj/best_modelseg.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'segautoenc' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "segautoenc.load_weights(\"/content/drive/My Drive/cvproj/best_modelseg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "t4w5tPepDAF1",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "model_json = segautoenc.to_json()\n",
    "with open(\"/content/drive/My Drive/cvproj/seg300.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "7DhKO4cmRnWp",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Yh-WIrUpioVb",
    "colab_type": "code",
    "outputId": "e1c2a411-b717-491a-8934-e2f2511c5a25",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 208, 208, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_sval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0YGFNcm_gsBr",
    "colab_type": "code",
    "outputId": "8815d943-41f5-4e77-a3d8-3debc26fcec7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1105.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 208, 208, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 208, 208, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 208, 208, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 104, 104, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 104, 104, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 104, 104, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 52, 52, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 13, 13, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 26, 26, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 26, 26, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 256)       590080    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 52, 52, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 52, 52, 128)       295040    \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 52, 52, 128)       147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 104, 104, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 104, 104, 64)      73792     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 104, 104, 64)      36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 208, 208, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 208, 208, 32)      18464     \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 208, 208, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 208, 208, 3)       867       \n",
      "=================================================================\n",
      "Total params: 6,916,547\n",
      "Trainable params: 6,916,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import optimizers\n",
    "K.clear_session()\n",
    "\n",
    "ainput_img = Input(shape=(image_size, image_size, 3)) \n",
    "sx = Conv2D(32, (3, 3), activation='relu', padding='same',input_shape=(image_size, image_size, 3))(ainput_img)\n",
    "sx = Conv2D(32, (3,3),activation='relu',padding='same')(sx)\n",
    "sx = MaxPooling2D((2, 2), padding='same')(sx)\n",
    "sx = Conv2D(64, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = Conv2D(64, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = MaxPooling2D((2, 2), padding='same')(sx)\n",
    "sx = Dropout(rate=.2)(sx)\n",
    "sx = Conv2D(128, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = MaxPooling2D((2, 2), padding='same')(sx)\n",
    "sx = Conv2D(256, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = Conv2D(256, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = MaxPooling2D((2, 2), padding='same')(sx)\n",
    "sx = Conv2D(512, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = Conv2D(512, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = UpSampling2D((2, 2))(sx)\n",
    "sx = Conv2D(256, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = Conv2D(256, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = UpSampling2D((2, 2))(sx)\n",
    "sx = Conv2D(128, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = Conv2D(128, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = UpSampling2D((2, 2))(sx)\n",
    "#sx = concatenate([sx1,sx], axis = 3)\n",
    "sx = Conv2D(64, (3, 3), activation='relu', padding='same')(sx)\n",
    "sx = Conv2D(64, (3, 3), activation='relu',padding='same')(sx)\n",
    "sx = UpSampling2D((2, 2))(sx)\n",
    "#sx = concatenate([sxb,sx], axis = 3)\n",
    "sx = Conv2D(32, (3,3),activation='relu',padding='same')(sx)\n",
    "sx = Conv2D(32, (3,3),activation='relu',padding='same')(sx)\n",
    "decoded = Conv2D(3, (3,3), activation='sigmoid', padding='same')(sx)\n",
    "autoencpro = Model(ainput_img,decoded)\n",
    "\n",
    "\n",
    "autoencpro.compile(optimizer=optimizers.SGD(lr=0.1),loss='mse',metrics=['accuracy'])\n",
    "autoencpro.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "wQ9l7BChk8nt",
    "colab_type": "code",
    "outputId": "9642bb91-5df9-475d-c3e8-ecaf21a44c6f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6871.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1489 samples, validate on 1470 samples\n",
      "Epoch 1/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.4156\n",
      "Epoch 00001: val_loss improved from inf to 0.07220, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0730 - acc: 0.4161 - val_loss: 0.0722 - val_acc: 0.4996\n",
      "Epoch 2/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.4984\n",
      "Epoch 00002: val_loss improved from 0.07220 to 0.07118, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 28s 19ms/sample - loss: 0.0718 - acc: 0.4977 - val_loss: 0.0712 - val_acc: 0.5004\n",
      "Epoch 3/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.4986\n",
      "Epoch 00003: val_loss improved from 0.07118 to 0.07056, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0711 - acc: 0.4979 - val_loss: 0.0706 - val_acc: 0.5004\n",
      "Epoch 4/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.4989\n",
      "Epoch 00004: val_loss improved from 0.07056 to 0.07020, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0707 - acc: 0.4979 - val_loss: 0.0702 - val_acc: 0.5004\n",
      "Epoch 5/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.4975\n",
      "Epoch 00005: val_loss improved from 0.07020 to 0.06993, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0704 - acc: 0.4979 - val_loss: 0.0699 - val_acc: 0.5004\n",
      "Epoch 6/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.4976\n",
      "Epoch 00006: val_loss improved from 0.06993 to 0.06971, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0702 - acc: 0.4979 - val_loss: 0.0697 - val_acc: 0.5004\n",
      "Epoch 7/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.4969\n",
      "Epoch 00007: val_loss improved from 0.06971 to 0.06950, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0700 - acc: 0.4979 - val_loss: 0.0695 - val_acc: 0.5004\n",
      "Epoch 8/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.4981\n",
      "Epoch 00008: val_loss improved from 0.06950 to 0.06924, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0697 - acc: 0.4979 - val_loss: 0.0692 - val_acc: 0.5004\n",
      "Epoch 9/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.4980\n",
      "Epoch 00009: val_loss improved from 0.06924 to 0.06894, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0693 - acc: 0.4979 - val_loss: 0.0689 - val_acc: 0.5004\n",
      "Epoch 10/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.4968\n",
      "Epoch 00010: val_loss improved from 0.06894 to 0.06805, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0685 - acc: 0.4979 - val_loss: 0.0681 - val_acc: 0.5004\n",
      "Epoch 11/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.4969\n",
      "Epoch 00011: val_loss improved from 0.06805 to 0.06633, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0671 - acc: 0.4978 - val_loss: 0.0663 - val_acc: 0.4995\n",
      "Epoch 12/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.4983\n",
      "Epoch 00012: val_loss improved from 0.06633 to 0.06346, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0639 - acc: 0.4972 - val_loss: 0.0635 - val_acc: 0.5002\n",
      "Epoch 13/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.4859\n",
      "Epoch 00013: val_loss improved from 0.06346 to 0.05450, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0590 - acc: 0.4855 - val_loss: 0.0545 - val_acc: 0.4776\n",
      "Epoch 14/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.3683\n",
      "Epoch 00014: val_loss improved from 0.05450 to 0.04767, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0569 - acc: 0.3676 - val_loss: 0.0477 - val_acc: 0.4332\n",
      "Epoch 15/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.3488\n",
      "Epoch 00015: val_loss improved from 0.04767 to 0.04240, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0508 - acc: 0.3501 - val_loss: 0.0424 - val_acc: 0.4076\n",
      "Epoch 16/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.3596\n",
      "Epoch 00016: val_loss did not improve from 0.04240\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0448 - acc: 0.3608 - val_loss: 0.0518 - val_acc: 0.4844\n",
      "Epoch 17/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.3919\n",
      "Epoch 00017: val_loss improved from 0.04240 to 0.03885, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0433 - acc: 0.3937 - val_loss: 0.0388 - val_acc: 0.4012\n",
      "Epoch 18/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.4084\n",
      "Epoch 00018: val_loss improved from 0.03885 to 0.03733, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0402 - acc: 0.4089 - val_loss: 0.0373 - val_acc: 0.4037\n",
      "Epoch 19/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.4204\n",
      "Epoch 00019: val_loss improved from 0.03733 to 0.03327, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0379 - acc: 0.4193 - val_loss: 0.0333 - val_acc: 0.4483\n",
      "Epoch 20/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.4245\n",
      "Epoch 00020: val_loss improved from 0.03327 to 0.03201, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0360 - acc: 0.4244 - val_loss: 0.0320 - val_acc: 0.4295\n",
      "Epoch 21/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.4172\n",
      "Epoch 00021: val_loss improved from 0.03201 to 0.03177, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0350 - acc: 0.4182 - val_loss: 0.0318 - val_acc: 0.4352\n",
      "Epoch 22/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.4230\n",
      "Epoch 00022: val_loss did not improve from 0.03177\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0334 - acc: 0.4227 - val_loss: 0.0442 - val_acc: 0.3559\n",
      "Epoch 23/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.4165\n",
      "Epoch 00023: val_loss improved from 0.03177 to 0.03120, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0330 - acc: 0.4163 - val_loss: 0.0312 - val_acc: 0.4671\n",
      "Epoch 24/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.4249\n",
      "Epoch 00024: val_loss did not improve from 0.03120\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0322 - acc: 0.4234 - val_loss: 0.0400 - val_acc: 0.4871\n",
      "Epoch 25/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.4230\n",
      "Epoch 00025: val_loss did not improve from 0.03120\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0320 - acc: 0.4221 - val_loss: 0.0388 - val_acc: 0.4828\n",
      "Epoch 26/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.4173\n",
      "Epoch 00026: val_loss did not improve from 0.03120\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0304 - acc: 0.4170 - val_loss: 0.0336 - val_acc: 0.4791\n",
      "Epoch 27/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.4229\n",
      "Epoch 00027: val_loss did not improve from 0.03120\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0304 - acc: 0.4229 - val_loss: 0.0380 - val_acc: 0.4839\n",
      "Epoch 28/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.4242\n",
      "Epoch 00028: val_loss did not improve from 0.03120\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0296 - acc: 0.4224 - val_loss: 0.0337 - val_acc: 0.4734\n",
      "Epoch 29/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.4162\n",
      "Epoch 00029: val_loss did not improve from 0.03120\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0290 - acc: 0.4158 - val_loss: 0.0357 - val_acc: 0.4781\n",
      "Epoch 30/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.4151\n",
      "Epoch 00030: val_loss did not improve from 0.03120\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0287 - acc: 0.4151 - val_loss: 0.0357 - val_acc: 0.4818\n",
      "Epoch 31/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.4208\n",
      "Epoch 00031: val_loss improved from 0.03120 to 0.03053, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0285 - acc: 0.4200 - val_loss: 0.0305 - val_acc: 0.4693\n",
      "Epoch 32/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.4197\n",
      "Epoch 00032: val_loss did not improve from 0.03053\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0285 - acc: 0.4189 - val_loss: 0.0364 - val_acc: 0.4779\n",
      "Epoch 33/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.4162\n",
      "Epoch 00033: val_loss did not improve from 0.03053\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0276 - acc: 0.4174 - val_loss: 0.0314 - val_acc: 0.4654\n",
      "Epoch 34/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.4159\n",
      "Epoch 00034: val_loss did not improve from 0.03053\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0266 - acc: 0.4154 - val_loss: 0.0333 - val_acc: 0.4753\n",
      "Epoch 35/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.4173\n",
      "Epoch 00035: val_loss did not improve from 0.03053\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0272 - acc: 0.4164 - val_loss: 0.0344 - val_acc: 0.4763\n",
      "Epoch 36/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.4178\n",
      "Epoch 00036: val_loss did not improve from 0.03053\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0266 - acc: 0.4178 - val_loss: 0.0371 - val_acc: 0.4814\n",
      "Epoch 37/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.4198\n",
      "Epoch 00037: val_loss did not improve from 0.03053\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0265 - acc: 0.4183 - val_loss: 0.0318 - val_acc: 0.4695\n",
      "Epoch 38/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.4113\n",
      "Epoch 00038: val_loss did not improve from 0.03053\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0262 - acc: 0.4110 - val_loss: 0.0361 - val_acc: 0.4794\n",
      "Epoch 39/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.4144\n",
      "Epoch 00039: val_loss did not improve from 0.03053\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0256 - acc: 0.4142 - val_loss: 0.0337 - val_acc: 0.4760\n",
      "Epoch 40/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.4149\n",
      "Epoch 00040: val_loss did not improve from 0.03053\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0261 - acc: 0.4149 - val_loss: 0.0308 - val_acc: 0.4678\n",
      "Epoch 41/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.4147\n",
      "Epoch 00041: val_loss did not improve from 0.03053\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0257 - acc: 0.4137 - val_loss: 0.0380 - val_acc: 0.4831\n",
      "Epoch 42/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.4105\n",
      "Epoch 00042: val_loss improved from 0.03053 to 0.02446, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0251 - acc: 0.4104 - val_loss: 0.0245 - val_acc: 0.4217\n",
      "Epoch 43/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.4079\n",
      "Epoch 00043: val_loss improved from 0.02446 to 0.02247, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0249 - acc: 0.4081 - val_loss: 0.0225 - val_acc: 0.4259\n",
      "Epoch 44/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.4134\n",
      "Epoch 00044: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0251 - acc: 0.4139 - val_loss: 0.0235 - val_acc: 0.4213\n",
      "Epoch 45/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.4119\n",
      "Epoch 00045: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0252 - acc: 0.4127 - val_loss: 0.0228 - val_acc: 0.4113\n",
      "Epoch 46/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.4074\n",
      "Epoch 00046: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0245 - acc: 0.4079 - val_loss: 0.0246 - val_acc: 0.4014\n",
      "Epoch 47/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.4110\n",
      "Epoch 00047: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0248 - acc: 0.4117 - val_loss: 0.0236 - val_acc: 0.4118\n",
      "Epoch 48/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.4105\n",
      "Epoch 00048: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0243 - acc: 0.4110 - val_loss: 0.0235 - val_acc: 0.4080\n",
      "Epoch 49/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.4111\n",
      "Epoch 00049: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0235 - acc: 0.4113 - val_loss: 0.0243 - val_acc: 0.4463\n",
      "Epoch 50/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.4125\n",
      "Epoch 00050: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0241 - acc: 0.4117 - val_loss: 0.0331 - val_acc: 0.4701\n",
      "Epoch 51/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.4126\n",
      "Epoch 00051: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0243 - acc: 0.4119 - val_loss: 0.0281 - val_acc: 0.4547\n",
      "Epoch 52/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.4060\n",
      "Epoch 00052: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0241 - acc: 0.4065 - val_loss: 0.0268 - val_acc: 0.4494\n",
      "Epoch 53/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.4066\n",
      "Epoch 00053: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0233 - acc: 0.4071 - val_loss: 0.0307 - val_acc: 0.4631\n",
      "Epoch 54/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.4057\n",
      "Epoch 00054: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0240 - acc: 0.4062 - val_loss: 0.0259 - val_acc: 0.4509\n",
      "Epoch 55/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.4069\n",
      "Epoch 00055: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0237 - acc: 0.4075 - val_loss: 0.0286 - val_acc: 0.4606\n",
      "Epoch 56/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.4124\n",
      "Epoch 00056: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0238 - acc: 0.4114 - val_loss: 0.0368 - val_acc: 0.4783\n",
      "Epoch 57/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.4066\n",
      "Epoch 00057: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0234 - acc: 0.4066 - val_loss: 0.0309 - val_acc: 0.4635\n",
      "Epoch 58/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.4086\n",
      "Epoch 00058: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0238 - acc: 0.4089 - val_loss: 0.0265 - val_acc: 0.4517\n",
      "Epoch 59/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.4062\n",
      "Epoch 00059: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0228 - acc: 0.4062 - val_loss: 0.0330 - val_acc: 0.4712\n",
      "Epoch 60/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.4082\n",
      "Epoch 00060: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0228 - acc: 0.4083 - val_loss: 0.0253 - val_acc: 0.4500\n",
      "Epoch 61/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.4089\n",
      "Epoch 00061: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0232 - acc: 0.4088 - val_loss: 0.0293 - val_acc: 0.4588\n",
      "Epoch 62/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.4094\n",
      "Epoch 00062: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0222 - acc: 0.4083 - val_loss: 0.0337 - val_acc: 0.4695\n",
      "Epoch 63/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.4015\n",
      "Epoch 00063: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0232 - acc: 0.4025 - val_loss: 0.0293 - val_acc: 0.4596\n",
      "Epoch 64/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.4114\n",
      "Epoch 00064: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0227 - acc: 0.4101 - val_loss: 0.0321 - val_acc: 0.4645\n",
      "Epoch 65/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.4016\n",
      "Epoch 00065: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0225 - acc: 0.4022 - val_loss: 0.0296 - val_acc: 0.4599\n",
      "Epoch 66/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.4101\n",
      "Epoch 00066: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0230 - acc: 0.4085 - val_loss: 0.0268 - val_acc: 0.4467\n",
      "Epoch 67/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.4048\n",
      "Epoch 00067: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0227 - acc: 0.4043 - val_loss: 0.0292 - val_acc: 0.4560\n",
      "Epoch 68/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.4031\n",
      "Epoch 00068: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0221 - acc: 0.4029 - val_loss: 0.0279 - val_acc: 0.4561\n",
      "Epoch 69/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.4077\n",
      "Epoch 00069: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0225 - acc: 0.4070 - val_loss: 0.0297 - val_acc: 0.4564\n",
      "Epoch 70/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.4070\n",
      "Epoch 00070: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0223 - acc: 0.4072 - val_loss: 0.0285 - val_acc: 0.4597\n",
      "Epoch 71/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.4102\n",
      "Epoch 00071: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0221 - acc: 0.4096 - val_loss: 0.0355 - val_acc: 0.4769\n",
      "Epoch 72/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.4044\n",
      "Epoch 00072: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0223 - acc: 0.4045 - val_loss: 0.0238 - val_acc: 0.4436\n",
      "Epoch 73/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.4055\n",
      "Epoch 00073: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0218 - acc: 0.4053 - val_loss: 0.0262 - val_acc: 0.4509\n",
      "Epoch 74/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.4084\n",
      "Epoch 00074: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0221 - acc: 0.4075 - val_loss: 0.0251 - val_acc: 0.4457\n",
      "Epoch 75/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.4043\n",
      "Epoch 00075: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0219 - acc: 0.4043 - val_loss: 0.0303 - val_acc: 0.4629\n",
      "Epoch 76/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.4064\n",
      "Epoch 00076: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0214 - acc: 0.4070 - val_loss: 0.0295 - val_acc: 0.4612\n",
      "Epoch 77/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.4036\n",
      "Epoch 00077: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0223 - acc: 0.4039 - val_loss: 0.0293 - val_acc: 0.4599\n",
      "Epoch 78/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.4059\n",
      "Epoch 00078: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0219 - acc: 0.4065 - val_loss: 0.0270 - val_acc: 0.4543\n",
      "Epoch 79/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.4050\n",
      "Epoch 00079: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0218 - acc: 0.4058 - val_loss: 0.0327 - val_acc: 0.4719\n",
      "Epoch 80/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.4102\n",
      "Epoch 00080: val_loss did not improve from 0.02247\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0224 - acc: 0.4097 - val_loss: 0.0257 - val_acc: 0.4483\n",
      "Epoch 81/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.4028\n",
      "Epoch 00081: val_loss improved from 0.02247 to 0.02247, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0199 - acc: 0.4032 - val_loss: 0.0225 - val_acc: 0.4403\n",
      "Epoch 82/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.4112\n",
      "Epoch 00082: val_loss improved from 0.02247 to 0.02168, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0217 - acc: 0.4103 - val_loss: 0.0217 - val_acc: 0.4292\n",
      "Epoch 83/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.4042\n",
      "Epoch 00083: val_loss did not improve from 0.02168\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0218 - acc: 0.4054 - val_loss: 0.0224 - val_acc: 0.4239\n",
      "Epoch 84/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.4070\n",
      "Epoch 00084: val_loss did not improve from 0.02168\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0215 - acc: 0.4080 - val_loss: 0.0234 - val_acc: 0.4368\n",
      "Epoch 85/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.4023\n",
      "Epoch 00085: val_loss improved from 0.02168 to 0.02063, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0203 - acc: 0.4020 - val_loss: 0.0206 - val_acc: 0.4110\n",
      "Epoch 86/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.4048\n",
      "Epoch 00086: val_loss improved from 0.02063 to 0.01990, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0217 - acc: 0.4046 - val_loss: 0.0199 - val_acc: 0.4216\n",
      "Epoch 87/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.4088\n",
      "Epoch 00087: val_loss did not improve from 0.01990\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0226 - acc: 0.4083 - val_loss: 0.0223 - val_acc: 0.4173\n",
      "Epoch 88/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.4033\n",
      "Epoch 00088: val_loss did not improve from 0.01990\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0206 - acc: 0.4037 - val_loss: 0.0214 - val_acc: 0.4289\n",
      "Epoch 89/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.4027\n",
      "Epoch 00089: val_loss did not improve from 0.01990\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0215 - acc: 0.4035 - val_loss: 0.0205 - val_acc: 0.4268\n",
      "Epoch 90/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.4105\n",
      "Epoch 00090: val_loss did not improve from 0.01990\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0210 - acc: 0.4104 - val_loss: 0.0222 - val_acc: 0.4294\n",
      "Epoch 91/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.4021\n",
      "Epoch 00091: val_loss did not improve from 0.01990\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0215 - acc: 0.4026 - val_loss: 0.0209 - val_acc: 0.4239\n",
      "Epoch 92/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.4059\n",
      "Epoch 00092: val_loss did not improve from 0.01990\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0209 - acc: 0.4056 - val_loss: 0.0201 - val_acc: 0.4167\n",
      "Epoch 93/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.4057\n",
      "Epoch 00093: val_loss did not improve from 0.01990\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0203 - acc: 0.4050 - val_loss: 0.0201 - val_acc: 0.4144\n",
      "Epoch 94/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.4063\n",
      "Epoch 00094: val_loss improved from 0.01990 to 0.01923, saving model to best_modelautoenc.hdf5\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0207 - acc: 0.4060 - val_loss: 0.0192 - val_acc: 0.4167\n",
      "Epoch 95/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.4044\n",
      "Epoch 00095: val_loss did not improve from 0.01923\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0217 - acc: 0.4051 - val_loss: 0.0198 - val_acc: 0.4197\n",
      "Epoch 96/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.4066\n",
      "Epoch 00096: val_loss did not improve from 0.01923\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0210 - acc: 0.4064 - val_loss: 0.0203 - val_acc: 0.4111\n",
      "Epoch 97/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.4099\n",
      "Epoch 00097: val_loss did not improve from 0.01923\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0203 - acc: 0.4107 - val_loss: 0.0212 - val_acc: 0.4283\n",
      "Epoch 98/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.4117\n",
      "Epoch 00098: val_loss did not improve from 0.01923\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0202 - acc: 0.4102 - val_loss: 0.0195 - val_acc: 0.4146\n",
      "Epoch 99/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.4053\n",
      "Epoch 00099: val_loss did not improve from 0.01923\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0217 - acc: 0.4062 - val_loss: 0.0213 - val_acc: 0.4201\n",
      "Epoch 100/100\n",
      "1472/1489 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.4076\n",
      "Epoch 00100: val_loss did not improve from 0.01923\n",
      "1489/1489 [==============================] - 27s 18ms/sample - loss: 0.0207 - acc: 0.4082 - val_loss: 0.0204 - val_acc: 0.4177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9cc018ddd8>"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getGen(x):\n",
    "    data_gen_args = dict(featurewise_center=True,\n",
    "                         featurewise_std_normalization=True,\n",
    "                         rotation_range=20,\n",
    "                         width_shift_range=0.1,\n",
    "                         height_shift_range=0.1,\n",
    "                         \n",
    "                         zoom_range=0.2)\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    image_datagen.fit(x)\n",
    "    generator = image_datagen.flow(\n",
    "    x,x,batch_size=16)\n",
    "    return generator\n",
    "  \n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "mc = ModelCheckpoint('best_modelautoenc.hdf5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
    "\n",
    "autoencpro.fit(x_train,x_train,batch_size=64,epochs=100,shuffle=True,\n",
    "                         validation_data=(x_val, x_val),\n",
    "                         callbacks=[TensorBoard(log_dir='/tmp/autoencoder'),es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "vgBhiKjkqe5w",
    "colab_type": "code",
    "outputId": "e1b95bc6-10bf-4bfc-be81-2f85510d9b3c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281.0
    }
   },
   "outputs": [
    {
     "metadata": {},
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "full =np.array(x_train, copy=True)\n",
    "np.random.shuffle(full)\n",
    "sel = full[:10]\n",
    "\n",
    "decoded_imgs = autoencpro.predict(sel)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1,n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(sel[i].reshape(208, 208,3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(208, 208,3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "plt.savefig(\"/content/drive/My Drive/cvproj/out.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Z6nonNPDy6bE",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "530a4f83-5d60-4daa-a904-f1dd86488e0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x7f66c9f0ee48>>"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segautoenc.summary"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "cvproj.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}